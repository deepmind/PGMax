{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5mS2eAQ52HG"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Intrinsic Innovation LLC.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycb3Y7TvtESU"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this block if running on colab.research.google.com\n",
        "# !pip install git+https://github.com/deepmind/PGMax.git\n",
        "# !wget https://raw.githubusercontent.com/deepmind/PGMax/main/examples/example_data/rcn.npz\n",
        "# !mkdir example_data\n",
        "# !mv rcn.npz  example_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LILn6smVeBWP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from jax import numpy as jnp\n",
        "from jax import tree_util\n",
        "from joblib import Memory\n",
        "from scipy.ndimage import maximum_filter\n",
        "from scipy.signal import fftconvolve\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "############\n",
        "# Load PGMax\n",
        "from pgmax import fgraph, fgroup, infer, vgroup\n",
        "\n",
        "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04DzBht7ppuG"
      },
      "source": [
        "# 1. Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8YV2u0p-qE7"
      },
      "source": [
        "A Recursive Cortical Model (RCN) is a neuroscience-inspired probabilistic generative model for vision\n",
        "published in [Science 2017](https://www.science.org/doi/10.1126/science.aag2612)\n",
        "which performance for object recognition and scene segmentation tasks are comparable to deep learning approaches while being interpretable and orders of magnitude more data efficient. \n",
        "\n",
        "In this notebook, we load a two-level RCN model pre-trained on a small subset of 20 training images of the MNIST train dataset. \n",
        "We test this model on 20 test images of the MNIST test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIFKwX9I-rhT"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "def fetch_mnist_dataset(test_size: int, seed: int = 5) -\u003e Tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns test images randomly sampled from the MNIST test dataset.\n",
        "\n",
        "  Args:\n",
        "    test_size: Desired number of test images.\n",
        "    seed: Random seed.\n",
        "\n",
        "  Returns:\n",
        "    test_set: An array containing test_size images from the MNIST test dataset.\n",
        "    test_labels: Corresponding labels for the test images.\n",
        "  \"\"\"\n",
        "  np.random.seed(seed)\n",
        "  num_per_class = test_size // 10\n",
        "\n",
        "  dataset = tfds.as_numpy(tfds.load(\"mnist\", split=\"test\", batch_size=-1))\n",
        "  print(\"Successfully downloaded the MNIST dataset\")\n",
        "\n",
        "  full_mnist_test_images = dataset[\"image\"]\n",
        "  full_mnist_test_labels = dataset[\"label\"].astype(\"int\")\n",
        "\n",
        "  test_set = []\n",
        "  test_labels = []\n",
        "  for i in range(10):\n",
        "    idxs = np.random.choice(\n",
        "        np.argwhere(full_mnist_test_labels == i)[:, 0], num_per_class\n",
        "    )\n",
        "    for idx in idxs:\n",
        "      img = full_mnist_test_images[idx].reshape(28, 28)\n",
        "      img_arr = jax.image.resize(image=img, shape=(112, 112), method=\"bicubic\")\n",
        "      img = jnp.pad(\n",
        "          img_arr,\n",
        "          pad_width=tuple([(p, p) for p in (44, 44)]),\n",
        "          mode=\"constant\",\n",
        "          constant_values=0,\n",
        "      )\n",
        "\n",
        "      test_set.append(img)\n",
        "      test_labels.append(i)\n",
        "\n",
        "  return np.array(test_set), np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDsDUt6B-swq"
      },
      "outputs": [],
      "source": [
        "train_size = test_size = 20\n",
        "test_set, test_labels = fetch_mnist_dataset(test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF0mgAHA-wFm"
      },
      "source": [
        "# 2. Load the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rW1i8Ub-y2S"
      },
      "source": [
        "We load a pre-trained rcn model that has been trained using the code [here](https://github.com/vicariousinc/science_rcn/tree/master/science_rcn). The details of the variables are.\n",
        "- train_set and train_labels - A sample of MNIST train dataset containing 100 train images and their labels.\n",
        "- frcs and edges - Used to represent the learned rcn graphical models.\n",
        "- suppression_masks and filters - Saved numpy arrays that are used to detect the presence or absence of an oriented/directed edge in an image. Please refer to the function get_bu_msg to see how they are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usTqvpMU-t-d"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "folder_name = \"example_data/\"\n",
        "data = np.load(open(folder_name + \"rcn.npz\", \"rb\"), allow_pickle=True)\n",
        "idxs = range(0, 100, 100 // train_size)\n",
        "\n",
        "train_set, train_labels, frcs, edges, suppression_masks, filters = (\n",
        "    data[\"train_set\"][idxs, :, :],\n",
        "    data[\"train_labels\"][idxs],\n",
        "    data[\"frcs\"][idxs],\n",
        "    data[\"edges\"][idxs],\n",
        "    data[\"suppression_masks\"],\n",
        "    data[\"filters\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY_xJkpS_D0A"
      },
      "source": [
        "We initialize the following hyper-parameters.\n",
        "- hps and vps - Horizontal and vertical pool sizes respectively for RCN models. This represents the radius of the window around a pool vertex. Thus, a pool vertex will be activated by an input pixel in a rectangle of size [2*hps+1, 2*vps+1].\n",
        "- num_orients - The number of different orientations at which edges are detected.\n",
        "- brightness_diff_threshold - The brightness level at a pixel at which we declare the presence of an edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gblIDuhi-0Ny"
      },
      "outputs": [],
      "source": [
        "hps, vps = 12, 12\n",
        "num_orients = filters.shape[0]\n",
        "brightness_diff_threshold = 40.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qvdfG2O_FnH"
      },
      "source": [
        "# 3. Visualize loaded model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy0kpbIj_IYD"
      },
      "source": [
        "In RCN, a learned model is a weighted graph. \n",
        "\n",
        "The weights (or the 'perturb_radius') constraints how the two vertices are allowed to vary during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqe0qrSX_FIE"
      },
      "outputs": [],
      "source": [
        "img_size = 200\n",
        "pad = 44\n",
        "img_idx = 4\n",
        "\n",
        "model_img = np.ones((200, 200))\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "frc, edge, train_img = frcs[img_idx], edges[img_idx], train_set[img_idx]\n",
        "ax[0].imshow(train_img[pad : 200 - pad, pad : 200 - pad], cmap=\"gray\")\n",
        "ax[0].axis(\"off\")\n",
        "ax[0].set_title(\"Example training image\", fontsize=40)\n",
        "\n",
        "for e in edge:\n",
        "  i1, i2, w = e  # The vertices for this edge along with the perturn radius.\n",
        "  f1, r1, c1 = frc[i1]\n",
        "  f2, r2, c2 = frc[i2]\n",
        "\n",
        "  model_img[r1, c1] = 0\n",
        "  model_img[r2, c2] = 0\n",
        "  ax[1].text(\n",
        "      (c1 + c2) // 2 - pad, (r1 + r2) // 2 - pad, str(w), color=\"green\", fontsize=25\n",
        "  )\n",
        "  ax[1].plot([c1 - pad, c2 - pad], [r1 - pad, r2 - pad], color=\"green\", linewidth=0.5)\n",
        "\n",
        "ax[1].axis(\"off\")\n",
        "ax[1].imshow(model_img[pad : 200 - pad, pad : 200 - pad], cmap=\"gray\")\n",
        "ax[1].set_title(\"Corresponding RCN template\", fontsize=40)\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvTEe5V_LlM"
      },
      "source": [
        "## 3.1 Visualize the filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdlKCNjX_RWI"
      },
      "source": [
        "The filters are used to detect the oriented edges on a given image. They are pre-computed using Gabor filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i87nv7N_Jr1"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for i in range(filters.shape[0]):\n",
        "  idx = np.unravel_index(i, (4, 4))\n",
        "  ax[idx].imshow(filters[i], cmap=\"gray\")\n",
        "  ax[idx].axis(\"off\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3t8MFLv_VT0"
      },
      "source": [
        "# 4. Make pgmax graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYjJ_NIg_Wwh"
      },
      "source": [
        "Converting the pre-learned RCN model to PGMax factor graph so as to run inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTTSrkS_Ykh"
      },
      "source": [
        "## 4.1 Make variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qslZvRxo_QGV"
      },
      "outputs": [],
      "source": [
        "assert frcs.shape[0] == edges.shape[0]\n",
        "\n",
        " # The number of pool choices for the different variables of the PGM.\n",
        "M = (2 * hps + 1) * (2 * vps + 1) \n",
        "\n",
        "variables_all_models = []\n",
        "for idx in range(frcs.shape[0]):\n",
        "  frc = frcs[idx]\n",
        "  variables_all_models.append(vgroup.NDVarArray(num_states=M, shape=(frc.shape[0],)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy4-sFzF_bW6"
      },
      "source": [
        "## 4.2 Make factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeP2o5zb_dR6"
      },
      "source": [
        "### 4.2.1 Pre-compute the valid configs for different perturb radius."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIfsGgEU_Z2G"
      },
      "outputs": [],
      "source": [
        "def valid_configs(r: int, hps: int, vps: int) -\u003e np.ndarray:\n",
        "  \"\"\"Returns the valid configurations for a factor given the perturb radius.\n",
        "\n",
        "  Args:\n",
        "    r: Peturb radius\n",
        "    hps: The horizontal pool size.\n",
        "    vps: The vertical pool size.\n",
        "\n",
        "  Returns:\n",
        "    An array of shape (num_valid_configs, 2) containing all valid configurations\n",
        "  \"\"\"\n",
        "  configs = []\n",
        "  for i, (r1, c1) in enumerate(\n",
        "    np.array(\n",
        "      np.unravel_index(\n",
        "        np.arange((2 * hps + 1) * (2 * vps + 1)), (2 * hps + 1, 2 * vps + 1)\n",
        "      )\n",
        "    ).T\n",
        "  ):\n",
        "      r2_min = max(r1 - r, 0)\n",
        "      r2_max = min(r1 + r, 2 * hps)\n",
        "      c2_min = max(c1 - r, 0)\n",
        "      c2_max = min(c1 + r, 2 * vps)\n",
        "      j = np.ravel_multi_index(\n",
        "          tuple(np.mgrid[r2_min : r2_max + 1, c2_min : c2_max + 1]),\n",
        "          (2 * hps + 1, 2 * vps + 1),\n",
        "      ).ravel()\n",
        "      configs.append(np.stack([np.full(j.shape, fill_value=i), j], axis=1))\n",
        "\n",
        "  return np.concatenate(configs)\n",
        "\n",
        "# The maximum perturb radius for which to pre-compute the valid configs.\n",
        "max_perturb_radius = 22\n",
        "valid_configs_list = [valid_configs(r, hps, vps) for r in range(max_perturb_radius)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BjHsvso_g3v"
      },
      "source": [
        "### 4.2.2 Make the factor graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORf_GBJa_eZ0"
      },
      "outputs": [],
      "source": [
        "fg = fgraph.FactorGraph(variables_all_models)\n",
        "\n",
        "# Adding rcn model edges to the pgmax factor graph.\n",
        "for idx in range(edges.shape[0]):\n",
        "  edge = edges[idx]\n",
        "\n",
        "  for e in edge:\n",
        "    i1, i2, r = e\n",
        "    factor_group = fgroup.EnumFactorGroup(\n",
        "        variables_for_factors=[\n",
        "            [variables_all_models[idx][i1], variables_all_models[idx][i2]]\n",
        "        ],\n",
        "        factor_configs=valid_configs_list[r],\n",
        "    )\n",
        "    fg.add_factors(factor_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376CO8xp_jqp"
      },
      "source": [
        "# 5. Run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdsq2HD3_lIh"
      },
      "source": [
        "## 5.1 Helper functions to initialize the evidence for a given image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTJvHIqi_h_8"
      },
      "outputs": [],
      "source": [
        "def get_bu_msg(img: np.ndarray) -\u003e np.ndarray:\n",
        "  \"\"\"Computes the bottom-up messages given a test image.\n",
        "\n",
        "  Args:\n",
        "    img: The rgb image to compute bottom up messages on [H, W, 3].\n",
        "\n",
        "  Returns:\n",
        "    An array of shape [16, H, W] denoting the presence or absence of an oriented \n",
        "    and directional line-segments at a particular location. \n",
        "    The elements of this array belong to the set {+1, -1}.\n",
        "  \"\"\"\n",
        "\n",
        "  # Convolving the image with different gabor filters.\n",
        "  filtered = np.zeros((filters.shape[0],) + img.shape, dtype=np.float32)\n",
        "  for i in range(filters.shape[0]):\n",
        "    kern = filters[i, :, :]\n",
        "    filtered[i] = fftconvolve(img, kern, mode=\"same\")\n",
        "\n",
        "  # Applying non-max suppression to all the filtered images.\n",
        "  localized = np.zeros_like(filtered)\n",
        "  cross_orient_max = filtered.max(0)\n",
        "  filtered[filtered \u003c 0] = 0\n",
        "  for i, (layer, suppress_mask) in enumerate(zip(filtered, suppression_masks)):\n",
        "    competitor_maxs = maximum_filter(layer, footprint=suppress_mask, mode=\"nearest\")\n",
        "    localized[i] = competitor_maxs \u003c= layer\n",
        "  localized[cross_orient_max \u003e filtered] = 0\n",
        "\n",
        "  # Threshold and binarize\n",
        "  localized *= (filtered / brightness_diff_threshold).clip(0, 1)\n",
        "  localized[localized \u003c 1] = 0\n",
        "\n",
        "  # Apply cross-channel pooling.\n",
        "  pooled_channels = [-np.ones_like(sf) for sf in localized]\n",
        "  for i, pc in enumerate(pooled_channels):\n",
        "    for channel_offset in [0, -1, 1]:\n",
        "      ch = (i + channel_offset) % num_orients\n",
        "      pos_chan = localized[ch]\n",
        "      np.maximum(pc, pos_chan, pc)\n",
        "\n",
        "  # Remapping the elements to set {+1, -1}.\n",
        "  bu_msg = np.array(pooled_channels)\n",
        "  bu_msg[bu_msg == 0] = -1\n",
        "  return bu_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJsY92w6_paY"
      },
      "source": [
        "### 5.1.1 Visualizing bu_msg for a sample image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lK2xZ4n_sBU"
      },
      "source": [
        "bu_msg has shape (16, H, W) where each 1 \u003c= f \u003c= 16 denotes the present or absense of a oriented edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U61Am7uD_nbH"
      },
      "outputs": [],
      "source": [
        "r_test_img = test_set[4]\n",
        "r_bu_msg = get_bu_msg(r_test_img)\n",
        "img = np.ones((200, 200))\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax[0].imshow(r_test_img, cmap=\"gray\")\n",
        "ax[0].axis(\"off\")\n",
        "ax[0].set_title(\"Input image\", fontsize=40)\n",
        "for i in range(r_bu_msg.shape[0]):\n",
        "  img[r_bu_msg[i] \u003e 0] = 0\n",
        "\n",
        "ax[1].imshow(img, cmap=\"gray\")\n",
        "ax[1].axis(\"off\")\n",
        "ax[1].set_title(\"Max filter response across 16 channels\", fontsize=40)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw4ZqEHm_ytd"
      },
      "source": [
        "### 5.2 Run MAP inference on all test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjVv_oFI_tQi"
      },
      "outputs": [],
      "source": [
        "def get_evidence(bu_msg: np.ndarray, frc: np.ndarray) -\u003e np.ndarray:\n",
        "  \"\"\"Returns the evidence, of shape (n_frcs, M).\n",
        "\n",
        "  Args:\n",
        "    bu_msg: Array of shape (n_features, 200, 200). Contains BU messages\n",
        "    frc: Array of shape (n_frcs, 3).\n",
        "  \"\"\"\n",
        "  evidence = np.zeros((frc.shape[0], M))\n",
        "  for v, (f, r, c) in enumerate(frc):\n",
        "    evidence[v] = bu_msg[f, r - hps : r + hps + 1, c - vps : c + vps + 1].ravel()\n",
        "  return evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knhvbN8q_w_4"
      },
      "outputs": [],
      "source": [
        "frcs_dict = {\n",
        "    variables_all_models[model_idx]: frcs[model_idx]\n",
        "    for model_idx in range(frcs.shape[0])\n",
        "}\n",
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")\n",
        "scores = np.zeros((len(test_set), frcs.shape[0]))\n",
        "map_states_dict = {}\n",
        "\n",
        "for test_idx in range(len(test_set)):\n",
        "  img = test_set[test_idx]\n",
        "\n",
        "  # Initializing evidence\n",
        "  bu_msg = get_bu_msg(img)\n",
        "  evidence_updates = jax.tree_util.tree_map(\n",
        "      lambda frc: get_evidence(bu_msg, frc), frcs_dict\n",
        "  )\n",
        "\n",
        "  # Max-product inference\n",
        "  start = time.time()\n",
        "  bp_arrays = bp.run(\n",
        "      bp.init(evidence_updates=evidence_updates),\n",
        "      num_iters=30,\n",
        "      temperature=0.0\n",
        "  )\n",
        "  map_states = infer.decode_map_states(bp.get_beliefs(bp_arrays))\n",
        "  end = time.time()\n",
        "  print(f\"Max product inference took {end-start:.3f} seconds for image {test_idx}\")\n",
        "\n",
        "  map_states_dict[test_idx] = map_states\n",
        "  score = tree_util.tree_map(\n",
        "      lambda evidence, map: jnp.sum(evidence[jnp.arange(map.shape[0]), map]),\n",
        "      evidence_updates,\n",
        "      map_states,\n",
        "  )\n",
        "  for model_idx in range(frcs.shape[0]):\n",
        "    scores[test_idx, model_idx] = score[variables_all_models[model_idx]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febR84RKDqSi"
      },
      "source": [
        "# 6. Compute metrics (accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F2hG78wDmco"
      },
      "outputs": [],
      "source": [
        "best_model_idx = np.argmax(scores, axis=1)\n",
        "test_preds = train_labels[best_model_idx]\n",
        "accuracy = (test_preds == test_labels).sum() / test_labels.shape[0]\n",
        "print(f\"accuracy = {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CTbGuP4Dt4O"
      },
      "source": [
        "# 7. Visualize predictions - backtrace for the top model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvOsQw1vDuFk"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(5, 4, figsize=(16, 20))\n",
        "for test_idx in range(20):\n",
        "  idx = np.unravel_index(test_idx, (5, 4))\n",
        "  map_state = map_states_dict[test_idx][\n",
        "      variables_all_models[best_model_idx[test_idx]]\n",
        "  ]\n",
        "  offsets = np.array(\n",
        "      np.unravel_index(map_state, (2 * hps + 1, 2 * vps + 1))\n",
        "  ).T - np.array([hps, vps])\n",
        "  \n",
        "  activations = frcs[best_model_idx[test_idx]][:, 1:] + offsets\n",
        "  for rd, cd in activations:\n",
        "    ax[idx].plot(cd, rd, \"r.\")\n",
        "\n",
        "  ax[idx].imshow(test_set[test_idx], cmap=\"gray\")\n",
        "  ax[idx].set_title(\n",
        "      f\"Ground Truth: {test_labels[test_idx]}, Pred: {test_preds[test_idx]}\",\n",
        "      fontsize=20,\n",
        "  )\n",
        "  ax[idx].axis(\"off\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}

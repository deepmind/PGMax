{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbdxDzMP5_kj"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Intrinsic Innovation LLC.\n",
        "# Copyright 2024 DeepMind Technologies Limited.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q2ZfbEO6G2Z"
      },
      "source": [
        "A GridMRF is an 8-connected, grid-arranged Markov random field with hidden variables, originally proposed in the [Query Training](https://ojs.aaai.org/index.php/AAAI/article/view/17004) AAAI 2021 paper.\n",
        "\n",
        "In this notebook we demonstrate inference and gradient-based learning of a GridMRF using PGMax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM7KS_RNtMgG"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this block if running on colab.research.google.com\n",
        "# !pip install git+https://github.com/deepmind/PGMax.git\n",
        "# !wget https://raw.githubusercontent.com/deepmind/PGMax/main/examples/example_data/gmrf_log_potentials.npz\n",
        "# !mkdir example_data\n",
        "# !mv gmrf_log_potentials.npz  example_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccA2OnvEqDar"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "from jax.example_libraries import optimizers\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "############\n",
        "# Load PGMax\n",
        "from pgmax import fgraph, fgroup, infer, vgroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaCxb3yrT9s8"
      },
      "source": [
        "# Create the noisy MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7UdIg-NT7xe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.morphology import binary_dilation\n",
        "\n",
        "def contour_mnist(X):\n",
        "  \"\"\"Extract the contours of the MNIST images.\"\"\"\n",
        "  X = X.astype(float) / 255.\n",
        "  # Contour are obtained by dilating the digits\n",
        "  X = (X > 0.5).astype(int)\n",
        "  s = np.zeros((3, 3, 3))\n",
        "  s[1, 1, :3] = 1\n",
        "  s[1, :3, 1] = 1\n",
        "  X += binary_dilation(X, s)\n",
        "  X[X == 1] = -1\n",
        "  X[X == 0] = 1\n",
        "  X[X == -1] = 0\n",
        "  contour_X = np.ones((X.shape[0], 30, 30), int)\n",
        "  contour_X[:, 1:-1, 1:-1] = X\n",
        "  return contour_X\n",
        "\n",
        "def add_noise_and_remove_contours(\n",
        "    images,\n",
        "    n_spurious_per_image,\n",
        "    p_contour_deletion,\n",
        "    n_add_contour_tries=1000,\n",
        "    seed=0\n",
        "):\n",
        "  \"\"\"Add spurious edges and removes contours at random.\"\"\"\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  noise_patterns = np.array([\n",
        "      [[1, 3, 1], [1, 3, 1], [1, 3, 1]],\n",
        "      [[1, 1, 1], [3, 3, 3], [1, 1, 1]],\n",
        "      [[3, 1, 1], [1, 3, 1], [1, 1, 3]],\n",
        "      [[1, 1, 3], [1, 3, 1], [3, 1, 1]]\n",
        "  ])\n",
        "  assert noise_patterns.shape[1:] == (3, 3)\n",
        "\n",
        "  N, H, W = images.shape\n",
        "  noisy_images = images.copy()\n",
        "\n",
        "  # Add spurious edges\n",
        "  for n in range(N):\n",
        "    n_spurious = 0\n",
        "    for t in range(n_add_contour_tries):\n",
        "      r, c = np.random.randint(H - 4), np.random.randint(W - 4)\n",
        "      if (noisy_images[n, r: r + 5, c: c + 5] == 1).all():  # all out\n",
        "        idx = np.random.randint(len(noise_patterns))\n",
        "        noisy_images[n, r + 1:r + 4, c + 1:c + 4] = noise_patterns[idx]\n",
        "        n_spurious += 1\n",
        "      if n_spurious == n_spurious_per_image:\n",
        "        break\n",
        "\n",
        "  # Remove contours\n",
        "  n, r, c = (noisy_images == 0).nonzero()\n",
        "  mask = np.random.binomial(1, p=p_contour_deletion, size=len(r)).astype(bool)\n",
        "  n, r, c = n[mask], r[mask], c[mask]\n",
        "  noisy_images[n, r, c] = 1\n",
        "\n",
        "  # Probability of observing 1 (contour) given label is 0 (border), 1 (out) 2(in)\n",
        "  p_contour = np.array([\n",
        "      1 - mask.mean(),\n",
        "      ((noisy_images == 3).sum() + 0.0) / ((noisy_images == 1).sum() + (noisy_images == 3).sum()),\n",
        "      1e-10\n",
        "  ])\n",
        "  noisy_images[noisy_images == 3] = 0\n",
        "  noisy_images[noisy_images > 0] = 1\n",
        "  return noisy_images, p_contour\n",
        "\n",
        "\n",
        "def get_noisy_mnist(dataset=\"test\", n_samples=100):\n",
        "  data = tfds.as_numpy(tfds.load(\"mnist\", split=dataset, batch_size=-1))\n",
        "  if n_samples is None:\n",
        "    X = data[\"image\"][:, :, :, 0]\n",
        "  else:\n",
        "    X = data[\"image\"][:n_samples, :, :, 0]\n",
        "  target_images = contour_mnist(X)\n",
        "  noisy_images, p_contour = add_noise_and_remove_contours(\n",
        "      target_images,\n",
        "      n_spurious_per_image=8,\n",
        "      p_contour_deletion=0.2\n",
        "  )\n",
        "  print(f\"Noisy {dataset} MNIST generated for {n_samples} samples\")\n",
        "  return target_images, noisy_images, p_contour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc6JJj4M6MEe"
      },
      "source": [
        "# Visualize a trained GridMRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPT4Vdru6Ili"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "target_images_test, noisy_images_test, p_contour = get_noisy_mnist(dataset=\"test\", n_samples=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFhZ5Ma69aPj"
      },
      "outputs": [],
      "source": [
        "# Load a pretrained large model\n",
        "folder_name = \"example_data/\"\n",
        "grmf_log_potentials = np.load(open(folder_name + \"gmrf_log_potentials.npz\", 'rb'), allow_pickle=True)\n",
        "\n",
        "# The number of clones defines the number of states of the categorical variables\n",
        "n_clones = grmf_log_potentials[\"n_clones\"]\n",
        "num_states = int(np.sum(n_clones))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHoMRxlW95gZ"
      },
      "outputs": [],
      "source": [
        "# Create the factor graph\n",
        "M, N = target_images_test.shape[-2:]\n",
        "variables = vgroup.NDVarArray(num_states=num_states, shape=(M, N))\n",
        "fg = fgraph.FactorGraph(variables)\n",
        "\n",
        "# Create top-down factors\n",
        "top_down = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii + 1, jj]]\n",
        "        for ii in range(M - 1)\n",
        "        for jj in range(N)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create left-right factors\n",
        "left_right = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii, jj + 1]]\n",
        "        for ii in range(M)\n",
        "        for jj in range(N - 1)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create diagonal factors\n",
        "diagonal0 = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii + 1, jj + 1]]\n",
        "        for ii in range(M - 1)\n",
        "        for jj in range(N - 1)\n",
        "    ],\n",
        ")\n",
        "diagonal1 = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=[\n",
        "        [variables[ii, jj], variables[ii - 1, jj + 1]]\n",
        "        for ii in range(1, M)\n",
        "        for jj in range(N - 1)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Add factors to the factor graph\n",
        "fg.add_factors([top_down, left_right, diagonal0, diagonal1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1A0xruQ97IR"
      },
      "outputs": [],
      "source": [
        "# Create the BP functions\n",
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")\n",
        "\n",
        "# We need this quantity to set the unaries\n",
        "p_contour_augmented = jax.device_put(np.repeat(p_contour, n_clones))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll5cfWv696w-"
      },
      "outputs": [],
      "source": [
        "def run_inference_and_plot(noisy_images, target_images, log_potentials, n_plots=5):\n",
        "  \"\"\"Run inference on 5 randomly selected images and plot the predictions.\"\"\"\n",
        "  fig, ax = plt.subplots(n_plots, 3, figsize=(7, 2 * n_plots))\n",
        "\n",
        "  indices = np.random.permutation(noisy_images.shape[0])[:n_plots]\n",
        "  for plot_idx, idx in tqdm(enumerate(indices), total=n_plots):\n",
        "    noisy_image = noisy_images[idx]\n",
        "    target_image = target_images[idx]\n",
        "\n",
        "    # Update the evidence\n",
        "    evidence = jnp.log(\n",
        "        jnp.where(\n",
        "            noisy_image[..., None] == 0,\n",
        "            p_contour_augmented,\n",
        "            1 - p_contour_augmented\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Run sum-product to estimate the marginaks\n",
        "    marginals = infer.get_marginals(\n",
        "        bp.get_beliefs(\n",
        "            bp.run(\n",
        "                bp.init(\n",
        "                    evidence_updates={variables: evidence},\n",
        "                    log_potentials_updates=log_potentials,\n",
        "                ),\n",
        "                num_iters=15,\n",
        "                damping=0.0,\n",
        "                temperature=1.0\n",
        "            )\n",
        "        )\n",
        "    )[variables]\n",
        "\n",
        "    # Look at the decoded image\n",
        "    pred_image = np.argmax(\n",
        "        np.stack(\n",
        "            [\n",
        "                np.sum(marginals[..., :-2], axis=-1),\n",
        "                marginals[..., -2],\n",
        "                marginals[..., -1],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        ),\n",
        "        axis=-1,\n",
        "    )\n",
        "    ax[plot_idx, 0].imshow(noisy_image)\n",
        "    ax[plot_idx, 0].axis(\"off\")\n",
        "    ax[plot_idx, 1].imshow(target_image)\n",
        "    ax[plot_idx, 1].axis(\"off\")\n",
        "    ax[plot_idx, 2].imshow(pred_image)\n",
        "    ax[plot_idx, 2].axis(\"off\")\n",
        "    if plot_idx == 0:\n",
        "      ax[plot_idx, 0].set_title(\"Input noisy image\", fontsize=18)\n",
        "      ax[plot_idx, 1].set_title(\"Ground truth\", fontsize=18)\n",
        "      ax[plot_idx, 2].set_title(\"GridMRF prediction\", fontsize=18)\n",
        "\n",
        "  fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference using the pretrained potentials\n",
        "log_potentials_pretrained = {\n",
        "    top_down: grmf_log_potentials[\"top_down\"],\n",
        "    left_right: grmf_log_potentials[\"left_right\"],\n",
        "    diagonal0: grmf_log_potentials[\"diagonal0\"],\n",
        "    diagonal1: grmf_log_potentials[\"diagonal1\"],\n",
        "}\n",
        "\n",
        "run_inference_and_plot(\n",
        "    noisy_images_test,\n",
        "    target_images_test,\n",
        "    log_potentials_pretrained\n",
        ")"
      ],
      "metadata": {
        "id": "vNjVtx4DKZX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXpcAG2q-BHn"
      },
      "source": [
        "# Finetune a perturbed model\n",
        "\n",
        "We now illustrate how we can train a small GridMRF.\n",
        "\n",
        "For this example to run fast, we do not initialize the parameters from scratch. Instead we perturb the pretrained parameters and finetune the model on a small number of 500 training samples for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need this quantity to compute the loss\n",
        "prototype_targets = jax.device_put(\n",
        "    np.array(\n",
        "        [\n",
        "            np.repeat(np.array([1, 0, 0]), n_clones),\n",
        "            np.repeat(np.array([0, 1, 0]), n_clones),\n",
        "            np.repeat(np.array([0, 0, 1]), n_clones),\n",
        "        ]\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "U75qiY3wK_W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYx6IoP3-AF2"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss(noisy_image, target_image, log_potentials):\n",
        "  \"\"\"Computes the cross-entropy loss between the predicted marginals and the ground truth.\"\"\"\n",
        "  target = prototype_targets[target_image]\n",
        "\n",
        "  # Update the evidence\n",
        "  evidence = jnp.log(\n",
        "      jnp.where(\n",
        "          noisy_image[..., None] == 0,\n",
        "          p_contour_augmented,\n",
        "          1 - p_contour_augmented\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Rum sum-product to estimate the marginals\n",
        "  marginals = infer.get_marginals(\n",
        "      bp.get_beliefs(\n",
        "          bp.run(\n",
        "              bp.init(\n",
        "                  evidence_updates={variables: evidence},\n",
        "                  log_potentials_updates=log_potentials,\n",
        "              ),\n",
        "              num_iters=15,\n",
        "              damping=0.0,\n",
        "              temperature=1.0\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Compute the cross-entropy loss\n",
        "  logp = jnp.mean(jnp.log(jnp.sum(target * marginals[variables], axis=-1)))\n",
        "  return -logp\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def batch_loss(noisy_images, target_images, log_potentials):\n",
        "  \"\"\"Averages the loss across multiple images.\"\"\"\n",
        "  return jnp.mean(\n",
        "      jax.vmap(loss, in_axes=(0, 0, None), out_axes=0)(\n",
        "          noisy_images, target_images, log_potentials\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDFJ0iph99bQ"
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnames=\"opt\")\n",
        "def update(log_potentials, batch_noisy_images, batch_target_images, opt, opt_state):\n",
        "  \"\"\"Update the log potentials.\"\"\"\n",
        "  loss, grad_log_potentials = jax.value_and_grad(batch_loss, argnums=2)(\n",
        "      batch_noisy_images, batch_target_images, log_potentials\n",
        "  )\n",
        "  updates, new_opt_state = opt.update(grad_log_potentials, opt_state, log_potentials)\n",
        "  new_log_potentials = optax.apply_updates(log_potentials, updates)\n",
        "  return loss, new_log_potentials, new_opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvR9T96j-EFU"
      },
      "outputs": [],
      "source": [
        "# Load the training data\n",
        "target_images_train, noisy_images_train, _ = get_noisy_mnist(dataset=\"train\", n_samples=500)\n",
        "\n",
        "# Perturb the pretrained potentials\n",
        "temp = 0.2\n",
        "log_potentials_finetuned = {\n",
        "    top_down: grmf_log_potentials[\"top_down\"] + temp * np.random.randn(num_states, num_states),\n",
        "    left_right: grmf_log_potentials[\"left_right\"] + temp * np.random.randn(num_states, num_states),\n",
        "    diagonal0: grmf_log_potentials[\"diagonal0\"] + temp * np.random.randn(num_states, num_states),\n",
        "    diagonal1: grmf_log_potentials[\"diagonal1\"] + temp * np.random.randn(num_states, num_states),\n",
        "}\n",
        "\n",
        "\n",
        "# Create the optimizer\n",
        "opt = optax.adam(learning_rate=3e-3)\n",
        "opt_state = opt.init(log_potentials_finetuned)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 10\n",
        "n_epochs = 1\n",
        "n_batches = noisy_images_train.shape[0] // batch_size\n",
        "\n",
        "losses = []\n",
        "with tqdm(total=n_epochs * n_batches) as pbar:\n",
        "  for epoch in range(n_epochs):\n",
        "    indices = np.random.permutation(noisy_images_train.shape[0])\n",
        "    for idx in range(n_batches):\n",
        "      batch_indices = indices[idx * batch_size : (idx + 1) * batch_size]\n",
        "      batch_noisy_images, batch_target_images = (\n",
        "          noisy_images_train[batch_indices],\n",
        "          target_images_train[batch_indices],\n",
        "      )\n",
        "      loss, log_potentials_finetuned, opt_state = update(\n",
        "          log_potentials_finetuned,\n",
        "          batch_noisy_images,\n",
        "          batch_target_images,\n",
        "          opt,\n",
        "          opt_state\n",
        "      )\n",
        "      pbar.update()\n",
        "      pbar.set_postfix(loss=loss)\n",
        "      losses.append(loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the cross-entropy losses\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Training iteration\", fontsize=16)\n",
        "plt.ylabel(\"Cross-entropy loss\", fontsize=16)"
      ],
      "metadata": {
        "id": "bk4sKzt3LYZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the inference results\n",
        "run_inference_and_plot(\n",
        "    noisy_images_test,\n",
        "    target_images_test,\n",
        "    log_potentials_finetuned\n",
        ")"
      ],
      "metadata": {
        "id": "Z5AVFt7nLaMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}
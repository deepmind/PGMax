{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c22Tmwt75jTj"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 DeepMind Technologies Limited.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBbZ9rmwswjx"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this block if running on colab.research.google.com\n",
        "# !pip install git+https://github.com/deepmind/PGMax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRekySEhZlwR"
      },
      "source": [
        "This notebook uses the Smooth Dual LP-MAP solver (1) to run inference on an Ising Model and compare its results with BP (2) to extract sparse feature activations from visually complex binary scenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LILn6smVeBWP"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.special import logit\n",
        "\n",
        "############\n",
        "# Load PGMax\n",
        "from pgmax import factor, fgraph, fgroup, infer, vgroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJpA83CAZz7R"
      },
      "source": [
        "# 1. Inference in an Ising model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9emhiPZZ4Tu"
      },
      "source": [
        "We reuse the Ising model from the example notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04DzBht7ppuG"
      },
      "source": [
        "### 1.1 Create the factor graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peLW-xjYeBur"
      },
      "outputs": [],
      "source": [
        "grid_size = 50\n",
        "\n",
        "variables = vgroup.NDVarArray(num_states=2, shape=(grid_size, grid_size))\n",
        "fg = fgraph.FactorGraph(variable_groups=variables)\n",
        "\n",
        "variables_for_factors = []\n",
        "for ii in range(grid_size):\n",
        "  for jj in range(grid_size):\n",
        "    kk = (ii + 1) % grid_size\n",
        "    ll = (jj + 1) % grid_size\n",
        "    variables_for_factors.append([variables[ii, jj], variables[kk, jj]])\n",
        "    variables_for_factors.append([variables[ii, jj], variables[ii, ll]])\n",
        "\n",
        "factor_group = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=variables_for_factors,\n",
        "    log_potential_matrix=0.8 * np.array([[1.0, -1.0], [-1.0, 1.0]]),\n",
        ")\n",
        "fg.add_factors(factor_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq7Z-aAzprQi"
      },
      "source": [
        "### 1.2 Run inference with BP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leQe02TlanSM"
      },
      "source": [
        "First, we run inference with Belief Propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqLfZzxhu3Vl"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "evidence_updates={variables: jax.random.gumbel(rng, shape=(grid_size, grid_size, 2))}\n",
        "num_iters = 2_000\n",
        "\n",
        "# Initialize the BP solver\n",
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")\n",
        "\n",
        "# Run inference\n",
        "bp_arrays = bp.init(evidence_updates=evidence_updates)\n",
        "_ = bp.run(bp_arrays, num_iters=num_iters, temperature=0)\n",
        "start = time.time()\n",
        "bp_arrays = bp.run(bp_arrays, num_iters=num_iters, temperature=0)\n",
        "bp_time = time.time() - start\n",
        "print(f\"BP time (after compiling): {bp_time:.4}s\")\n",
        "\n",
        "# Get the BP decoding\n",
        "beliefs = bp.get_beliefs(bp_arrays)\n",
        "bp_decoding = infer.decode_map_states(beliefs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_P6m57Qajn_"
      },
      "source": [
        "### 1.3 Run inference with SDLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsI2hYyCcxNy"
      },
      "source": [
        "Second, we run inference with the Smooth Dual LP solver using the same evidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-EBigM2arbi"
      },
      "source": [
        "Note that both the LP and SDLP solvers support the same interface, which is initialized via:\n",
        "```\n",
        "infer.build_inferer(fg.bp_state, backend=backend)\n",
        "```\n",
        "\n",
        "For the same number of iterations, the timings of BP and SDLP are comparable. However in practice BP needs fewer iterations than SDLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eopIok6PaiFx"
      },
      "outputs": [],
      "source": [
        "# Initialize the SDLP solver\n",
        "sdlp = infer.build_inferer(fg.bp_state, backend=\"sdlp\")\n",
        "\n",
        "# Run inference\n",
        "sdlp_arrays = sdlp.init(evidence_updates=evidence_updates)\n",
        "_ = sdlp.run(sdlp_arrays, logsumexp_temp=1e-3, num_iters=num_iters)\n",
        "\n",
        "start = time.time()\n",
        "sdlp_arrays, smooth_dual_objvals = sdlp.run_with_objvals(\n",
        "    sdlp_arrays,\n",
        "    logsumexp_temp=1e-3,\n",
        "    num_iters=num_iters\n",
        ")\n",
        "sdlp_time = time.time() - start\n",
        "print(f\"SDLP time (after compiling): {sdlp_time:.4}s\")\n",
        "\n",
        "# Get the SDLP decoding\n",
        "sdlp_beliefs = bp.get_beliefs(sdlp_arrays)\n",
        "sdlp_unaries_decoding = infer.decode_map_states(sdlp_beliefs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4SaZR7lccd4"
      },
      "source": [
        "The SDLP solver gives access to (1) an upper bound of the optimal objective value of the relaxed LP-MAP problem and (2) a lower bound of the optimal objective of the MAP problem. If both bounds are equal, then the LP relaxation is tight and we are at the MAP solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXdrTwcMcL-f"
      },
      "outputs": [],
      "source": [
        "primal_upper_bound = sdlp.get_primal_upper_bound(sdlp_arrays)\n",
        "print(f\"Upper bound for LP-MAP {primal_upper_bound:.3f}\")\n",
        "\n",
        "primal_lower_bound = sdlp.get_map_lower_bound(sdlp_arrays, sdlp_unaries_decoding)\n",
        "print(f\"Lower bound for MAP {primal_lower_bound:.3f}\")\n",
        "\n",
        "print(f\"Gap: {(100 * abs(primal_upper_bound - primal_lower_bound) / abs(primal_upper_bound)):.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiiov6OhdQ26"
      },
      "source": [
        "We plot the Smooth Dual objective value at each gradient step to visualize its convergence.\n",
        "\n",
        "We additionally vary the temperature and observe that at a high temperature, the objective value converge faster, but it is farther away from the MAP optimal objective value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaURAeaJdQUi"
      },
      "outputs": [],
      "source": [
        "# Solve the dual for a higher log-sum-exp temperature\n",
        "_, smooth_dual_objvals_higher = sdlp.run_with_objvals(\n",
        "    sdlp.init(evidence_updates=evidence_updates),\n",
        "    logsumexp_temp=1e-2,\n",
        "    num_iters=num_iters\n",
        ")\n",
        "# Solve the dual for a log-sum-exp temperature of 0\n",
        "_, smooth_dual_objvals_T0 = sdlp.run_with_objvals(\n",
        "    sdlp.init(evidence_updates=evidence_updates),\n",
        "    logsumexp_temp=0.0,  # subgradient descent\n",
        "    num_iters=num_iters\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(smooth_dual_objvals, label=\"SDLP objval for T=1e-3\", c='b')\n",
        "plt.scatter(num_iters, primal_upper_bound, c='g', s=200, marker=\"*\", label=\"MAP upper bound from the solution at T=1e-3\")\n",
        "plt.scatter(num_iters, primal_lower_bound, c='r', s=100, marker=\"x\", label=\"MAP lower bound from the solution at T=1e-3\")\n",
        "plt.plot(smooth_dual_objvals_higher, label=\"SDLP objval for T=1e-2\", c='g', linewidth=0.5)\n",
        "plt.plot(smooth_dual_objvals_T0, label=\"SDLP objval for T=0 (with SGD)\", c='r', linewidth=0.5)\n",
        "\n",
        "plt.legend(fontsize=16)\n",
        "plt.xlabel(\"Gradient steps\", fontsize=16)\n",
        "plt.ylabel(\"Objective value\", fontsize=16)\n",
        "_ = plt.title(\"Smooth Dual LP MAP for an Ising model\", fontsize=18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNiOlx0v53n_"
      },
      "source": [
        "### 1.4 Compare the BP and the SDLP solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp9nQgUuqX_D"
      },
      "source": [
        "We plot the BP and SDLP decodings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ9qX7SbeT8e"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
        "ax[0].imshow(bp_decoding[variables])\n",
        "ax[0].axis(\"off\")\n",
        "ax[0].set_title(\"BP decoding\", fontsize=18)\n",
        "\n",
        "ax[1].imshow(sdlp_unaries_decoding[variables])\n",
        "ax[1].axis(\"off\")\n",
        "ax[1].set_title(\"Dual LP decoding\", fontsize=18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59IWoN34qMcj"
      },
      "source": [
        "We compare the energies of the BP and SDLP solutions (lower is better)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2hM9Kw6dgfp"
      },
      "outputs": [],
      "source": [
        "bp_energy = infer.compute_energy(fg.bp_state, bp_arrays, bp_decoding)[0]\n",
        "sdlp_energy = - primal_lower_bound\n",
        "\n",
        "print(f\"Energy of the BP decoding: {bp_energy:.3f}\")\n",
        "print(f\"Energy of the Smooth Dual LP decoding: {sdlp_energy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR3pefg_lKei"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjm0hlb5nOyL"
      },
      "source": [
        "### 1.5 (Optional) Compare with the primal LP-MAP solver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PfoAss_nWrC"
      },
      "source": [
        "Last, we use the LP solver `cvxpy` to solve the LP-MAP problem to optimality.\n",
        "The LP solver is much slower than the other methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cTEYGN9drDY"
      },
      "outputs": [],
      "source": [
        "from pgmax.utils import primal_lp\n",
        "\n",
        "start = time.time()\n",
        "cvxpy_lp_vgroups_solution, cvxpy_lp_objval = primal_lp.primal_lp_solver(fg, evidence_updates)\n",
        "cvxpy_time = time.time() - start\n",
        "print(f\"Primal LP-MAP time: {cvxpy_time:.4}s\")\n",
        "\n",
        "# The optimal objective value is lower than the upper bound derived from the dual solution\n",
        "assert cvxpy_lp_objval <= primal_upper_bound\n",
        "\n",
        "decoding_from_cvxpy = infer.decode_map_states(cvxpy_lp_vgroups_solution)\n",
        "energy_from_cvpy = infer.compute_energy(fg.bp_state, bp_arrays, decoding_from_cvxpy)[0]\n",
        "print(f\"Energy from the optimal LP decoding: {energy_from_cvpy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZOu0HQkqKmI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qVYbVI1oWtU"
      },
      "source": [
        "# 2. Sparsification of a binary scene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97-BQmWWokWF"
      },
      "source": [
        "We use the SDLP solver to sparsify the scenes of the PMP Binary Deconvolution notebook example.\n",
        "\n",
        "We assume that the binary features W are known and we try to recover the binary indicator S from the binary scenes X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw4tsf_Wo2yq"
      },
      "source": [
        "### 2.1 Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxENEwvTogpZ"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this block if running on colab.research.google.com\n",
        "# !wget https://raw.githubusercontent.com/deepmind/PGMax/main/examples/example_data/conv_problem.npz\n",
        "# !mkdir example_data\n",
        "# !mv conv_problem.npz  example_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ihnobxgo2IT"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, display=True, nr=None):\n",
        "  \"Useful function for visualizing several images.\"\n",
        "  n_images, H, W = images.shape\n",
        "  images = images - images.min()\n",
        "  images /= images.max() + 1e-10\n",
        "\n",
        "  if nr is None:\n",
        "    nr = nc = np.ceil(np.sqrt(n_images)).astype(int)\n",
        "  else:\n",
        "    nc = n_images // nr\n",
        "    assert n_images == nr * nc\n",
        "  big_image = np.ones(((H + 1) * nr + 1, (W + 1) * nc + 1, 3))\n",
        "  big_image[..., :3] = 0\n",
        "  big_image[:: H + 1] = [0.5, 0, 0.5]\n",
        "\n",
        "  im = 0\n",
        "  for r in range(nr):\n",
        "    for c in range(nc):\n",
        "      if im < n_images:\n",
        "        big_image[\n",
        "            (H + 1) * r + 1 : (H + 1) * r + 1 + H,\n",
        "            (W + 1) * c + 1 : (W + 1) * c + 1 + W,\n",
        "            :,\n",
        "        ] = images[im, :, :, None]\n",
        "        im += 1\n",
        "\n",
        "  if display:\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(big_image, interpolation=\"none\")\n",
        "    plt.axis(\"off\")\n",
        "  return big_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJOrZdknoWKZ"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "folder_name = \"example_data/\"\n",
        "data = np.load(open(folder_name + \"conv_problem.npz\", 'rb'), allow_pickle=True)\n",
        "W_gt = data[\"W\"]\n",
        "X_gt = data[\"X\"]\n",
        "X_gt = X_gt[:20]\n",
        "\n",
        "_ = plot_images(X_gt[:8, 0], nr=2)\n",
        "plt.title(\"Convolved images\", fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnUdyJhXpCw8"
      },
      "source": [
        "### 2.2 Create the factor graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MJB963pqcy1"
      },
      "source": [
        "We use a similar factor graph as in the Binary Deconvolution notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31h2oHk8oWMo"
      },
      "outputs": [],
      "source": [
        "_, n_feat, feat_height, feat_width = W_gt.shape\n",
        "n_images, n_chan, im_height, im_width = X_gt.shape\n",
        "s_height = im_height - feat_height + 1\n",
        "s_width = im_width - feat_width + 1\n",
        "\n",
        "# Binary features\n",
        "W = vgroup.NDVarArray(num_states=2, shape=(n_chan, n_feat, feat_height, feat_width))\n",
        "\n",
        "# Binary indicators of features locations\n",
        "S = vgroup.NDVarArray(num_states=2, shape=(n_images, n_feat, s_height, s_width))\n",
        "\n",
        "# Auxiliary binary variables combining W and S\n",
        "SW = vgroup.NDVarArray(\n",
        "    num_states=2,\n",
        "    shape=(n_images, n_chan, im_height, im_width, n_feat, feat_height, feat_width),\n",
        ")\n",
        "\n",
        "# Binary images obtained by convolution\n",
        "X = vgroup.NDVarArray(num_states=2, shape=X_gt.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga4a1LP-qj1b"
      },
      "outputs": [],
      "source": [
        "# Factor graph\n",
        "fg = fgraph.FactorGraph(variable_groups=[S, W, SW, X])\n",
        "\n",
        "# Define the ANDFactors\n",
        "variables_for_ANDFactors = []\n",
        "variables_for_ORFactors_dict = defaultdict(list)\n",
        "for idx_img in tqdm(range(n_images)):\n",
        "  for idx_chan in range(n_chan):\n",
        "    for idx_s_height in range(s_height):\n",
        "      for idx_s_width in range(s_width):\n",
        "        for idx_feat in range(n_feat):\n",
        "          for idx_feat_height in range(feat_height):\n",
        "            for idx_feat_width in range(feat_width):\n",
        "              idx_img_height = idx_feat_height + idx_s_height\n",
        "              idx_img_width = idx_feat_width + idx_s_width\n",
        "              SW_var = SW[\n",
        "                  idx_img,\n",
        "                  idx_chan,\n",
        "                  idx_img_height,\n",
        "                  idx_img_width,\n",
        "                  idx_feat,\n",
        "                  idx_feat_height,\n",
        "                  idx_feat_width,\n",
        "              ]\n",
        "\n",
        "              variables_for_ANDFactor = [\n",
        "                  S[idx_img, idx_feat, idx_s_height, idx_s_width],\n",
        "                  W[idx_chan, idx_feat, idx_feat_height, idx_feat_width],\n",
        "                  SW_var,\n",
        "              ]\n",
        "              variables_for_ANDFactors.append(variables_for_ANDFactor)\n",
        "\n",
        "              X_var = X[idx_img, idx_chan, idx_img_height, idx_img_width]\n",
        "              variables_for_ORFactors_dict[X_var].append(SW_var)\n",
        "\n",
        "# Add ANDFactorGroup, which is computationally efficient\n",
        "AND_factor_group = fgroup.ANDFactorGroup(variables_for_ANDFactors)\n",
        "fg.add_factors(AND_factor_group)\n",
        "\n",
        "# Define the ORFactors\n",
        "variables_for_ORFactors = [\n",
        "    list(tuple(variables_for_ORFactors_dict[X_var]) + (X_var,))\n",
        "    for X_var in variables_for_ORFactors_dict\n",
        "]\n",
        "\n",
        "# Add ORFactorGroup, which is computationally efficient\n",
        "OR_factor_group = fgroup.ORFactorGroup(variables_for_ORFactors)\n",
        "fg.add_factors(OR_factor_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opoeY5-8rEZf"
      },
      "source": [
        "### 2.3 Run inference with SDLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spvEGFyvqj7_"
      },
      "outputs": [],
      "source": [
        "sdlp = infer.build_inferer(fg.bp_state, backend=\"sdlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZcee-nJrUBB"
      },
      "source": [
        "We define the unaries to address the posterior sampling query *p(S | X, W)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFpIVbU0rRvq"
      },
      "outputs": [],
      "source": [
        "pS = 1e-5\n",
        "pX = 1e-100\n",
        "\n",
        "# Unaries for the known W and X\n",
        "uW = np.zeros((W.shape) + (2,))\n",
        "uW[..., 0] = (2 * W_gt - 1) * logit(pX)\n",
        "\n",
        "uX = np.zeros((X_gt.shape) + (2,))\n",
        "uX[..., 0] = (2 * X_gt - 1) * logit(pX)\n",
        "\n",
        "# Sparsity inducing prior for S\n",
        "uS = np.zeros((S.shape) + (2,))\n",
        "uS[..., 1] = logit(pS)\n",
        "\n",
        "evidence_updates={\n",
        "    S: uS + jax.random.gumbel(rng, shape=uS.shape),\n",
        "    W: uW,\n",
        "    SW: np.zeros(shape=SW.shape),\n",
        "    X: uX,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe6tJbsH0-Q0"
      },
      "source": [
        "Run SDLP inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbeX5P9NrI16"
      },
      "outputs": [],
      "source": [
        "sdlp_arrays = sdlp.init(evidence_updates=evidence_updates)\n",
        "sdlp_arrays = sdlp.run(sdlp_arrays, num_iters=2_000, logsumexp_temp=0.001)\n",
        "primal_unaries_decoded, _ = sdlp.decode_primal_unaries(sdlp_arrays)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI2-nxmksQmB"
      },
      "source": [
        "### 3.3 Compute the reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF6LBi5Qthyx"
      },
      "source": [
        "We compute the recontructed scenes and the reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH-5_-Yms3Fc"
      },
      "outputs": [],
      "source": [
        "def or_layer(S, W):\n",
        "  \"\"\"2D convolution of S and W.\"\"\"\n",
        "  _, n_feat, s_height, s_width = S.shape\n",
        "  _, n_feat, feat_height, feat_width = W.shape\n",
        "  im_height, im_width = s_height + feat_height - 1, s_width + feat_width - 1\n",
        "\n",
        "  # Revert the features to have the proper orientations\n",
        "  Wrev = W[:, :, ::-1, ::-1]\n",
        "\n",
        "  # Pad the binary indicators\n",
        "  Spad = jax.lax.pad(\n",
        "      S,\n",
        "      0.0,\n",
        "      (\n",
        "          (0, 0, 0), # first dim of W\n",
        "          (0, 0, 0),\n",
        "          (feat_height - 1, feat_height - 1, 0),\n",
        "          (feat_width - 1, feat_width - 1, 0), # last dim of W\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  # Convolve Spad and W\n",
        "  def compute_sample(Spad1):\n",
        "    def compute_pixel(r, c):\n",
        "      X1 = (\n",
        "          1\n",
        "          - jax.lax.dynamic_slice(Spad1, (0, r, c), (n_feat, feat_height, feat_width))\n",
        "          * Wrev\n",
        "      ).prod((1, 2, 3))\n",
        "      return 1 - X1\n",
        "\n",
        "    compute_cols = jax.vmap(compute_pixel, in_axes=(None, 0), out_axes=1)\n",
        "    compute_rows_cols = jax.vmap(compute_cols, in_axes=(0, None), out_axes=1)\n",
        "    return compute_rows_cols(jnp.arange(im_height), jnp.arange(im_width))\n",
        "\n",
        "  return jax.vmap(compute_sample, in_axes=0, out_axes=0)(Spad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZlgzxKqsLVg"
      },
      "outputs": [],
      "source": [
        "X_rec = or_layer(primal_unaries_decoded[S].astype(float), W_gt)\n",
        "\n",
        "rec_ratio = np.abs(X_gt != X_rec).sum() / X_gt.size\n",
        "print(f\"Reconstruction error: {(100 * rec_ratio):.3f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UDUSPxIsehu"
      },
      "outputs": [],
      "source": [
        "_ = plot_images(X_rec[:, 0], nr=4)\n",
        "plt.title(\"Reconstructed images\", fontsize=18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvPRG4hk1ivx"
      },
      "outputs": [],
      "source": [
        "_ = plot_images(X_gt[:, 0], nr=4)\n",
        "plt.title(\"Original images\", fontsize=18)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcvVXX6C5wyO"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Intrinsic Innovation LLC.\n",
        "# Copyright 2023 DeepMind Technologies Limited.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrQCQyMyspeJ"
      },
      "source": [
        "[Restricted Boltzmann Machine (RBM)](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine) is a well-known and widely used PGM for learning probabilistic distributions over binary data. We demonstrate how we can easily implement [perturb-and-max-product (PMP)](https://proceedings.neurips.cc/paper/2021/hash/07b1c04a30f798b5506c1ec5acfb9031-Abstract.html) sampling from an RBM trained on MNIST digits using PGMax. PMP is a recently proposed method for approximately sampling from a PGM by computing the maximum-a-posteriori (MAP) configuration (using max-product LBP) of a perturbed version of the model.\n",
        "\n",
        "We start by making some necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hDkkNCKtAAu"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this block if running on colab.research.google.com\n",
        "# !pip install git+https://github.com/deepmind/PGMax.git\n",
        "# !wget https://raw.githubusercontent.com/deepmind/PGMax/main/examples/example_data/rbm_mnist.npz\n",
        "# !mkdir example_data\n",
        "# !mv rbm_mnist.npz  example_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LILn6smVeBWP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import functools\n",
        "\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "############\n",
        "# Load PGMax\n",
        "from pgmax import fgraph, fgroup, infer, vgroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04DzBht7ppuG"
      },
      "source": [
        "The [`pgmax.fgraph`](https://pgmax.readthedocs.io/en/latest/api.html#module-pgmax.fgraph) module contains classes for specifying factor graphs, the [`pgmax.vgroup`](https://pgmax.readthedocs.io/en/latest/api.html#module-pgmax.vgroup) module contains classes for specifying groups of variables, the [`pgmax.fgroup`](https://pgmax.readthedocs.io/en/latest/api.html#module-pgmax.fgroup) module contains classes for specifying groups of factors and the [`pgmax.infer`](https://pgmax.readthedocs.io/en/latest/api.html#module-pgmax.infer) module contains functions to perform LBP.\n",
        "\n",
        "Next, we load the RBM trained in Sec. 5.5 of the [PMP paper](https://proceedings.neurips.cc/paper/2021/hash/07b1c04a30f798b5506c1ec5acfb9031-Abstract.html) which has been trained on MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peLW-xjYeBur"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "folder_name = \"example_data/\"\n",
        "params = np.load(open(folder_name + \"rbm_mnist.npz\", 'rb'), allow_pickle=True)\n",
        "bv = params[\"bv\"]\n",
        "bh = params[\"bh\"]\n",
        "W = params[\"W\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W7C96g2vp-k"
      },
      "source": [
        "We now build the RBM factor graph using PGMax.\n",
        "\n",
        "First, we use [`NDVarArray`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.vgroup.NDVarArray), a convenient class for specifying a group of variables living on a multidimensional grid with possibly different number of states. This class shares some similarities with [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html), in particular regarding variables indexing. \n",
        "\n",
        "Second, we initialize the [`FactorGraph`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.fgraph.FactorGraph) `fg` with the set of defined [`VarGroup`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.vgroup.VarGroup)s. Once initialized, the set of variables in `fg` is fixed and cannot be changed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XxdeH0puB6F"
      },
      "outputs": [],
      "source": [
        "# Initialize factor graph\n",
        "hidden_variables = vgroup.NDVarArray(num_states=2, shape=bh.shape)\n",
        "visible_variables = vgroup.NDVarArray(num_states=2, shape=bv.shape)\n",
        "fg = fgraph.FactorGraph(variable_groups=[hidden_variables, visible_variables])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D4t9vPpvvjD"
      },
      "source": [
        "After initialization, `fg` does not have any factors. PGMax implements convenient and computationally efficient [`FactorGroup`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.fgroup.FactorGroup) for representing groups of similar [`Factor`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.factor.Factor)s. We then create the unary and pairwise factors using the [`EnumFactorGroup`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.fgroup.EnumFactorGroup) and [`PairwiseFactorGroup`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.fgroup.PairwiseFactorGroup) classes, before adding them to `fg`.\n",
        "\n",
        "Note that a [`FactorGroup`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.fgroup.FactorGroup) takes as argument `variables_for_factors` which is a list of lists of the variables involved in the different factors, and additional specific arguments (e.g. `factor_configs` and `log_potential_matrix` here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gerj92Vvrk7"
      },
      "outputs": [],
      "source": [
        "# Create unary factors\n",
        "hidden_unaries = fgroup.EnumFactorGroup(\n",
        "    variables_for_factors=[[hidden_variables[ii]] for ii in range(bh.shape[0])],\n",
        "    factor_configs=np.arange(2)[:, None],\n",
        "    log_potentials=np.stack([np.zeros_like(bh), bh], axis=1),\n",
        ")\n",
        "visible_unaries = fgroup.EnumFactorGroup(\n",
        "    variables_for_factors=[[visible_variables[jj]] for jj in range(bv.shape[0])],\n",
        "    factor_configs=np.arange(2)[:, None],\n",
        "    log_potentials=np.stack([np.zeros_like(bv), bv], axis=1),\n",
        ")\n",
        "\n",
        "# Create pairwise factors\n",
        "log_potential_matrix = np.zeros(W.shape + (2, 2)).reshape((-1, 2, 2))\n",
        "log_potential_matrix[:, 1, 1] = W.ravel()\n",
        "\n",
        "variables_for_factors = [\n",
        "    [hidden_variables[ii], visible_variables[jj]]\n",
        "    for ii in range(bh.shape[0])\n",
        "    for jj in range(bv.shape[0])\n",
        "]\n",
        "pairwise_factors = fgroup.PairwiseFactorGroup(\n",
        "    variables_for_factors=variables_for_factors,\n",
        "    log_potential_matrix=log_potential_matrix,\n",
        ")\n",
        "\n",
        "# Add factors to the FactorGraph\n",
        "fg.add_factors([hidden_unaries, visible_unaries, pairwise_factors])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ3u-tYgv0Ew"
      },
      "source": [
        "An alternative way displayed below adds the above [`Factor`](https://pgmax.readthedocs.io/en/latest/api.html#pgmax.factor.Factor)s iteratively without building the FactorGroup. This approach is not recommended as it can be much slower than using [`FactorGroup`](https://pgmax.readthedocs.io/en/latest/_autosummary/pgmax.fgroup.fgroup.FactorGroup.html#pgmax.fgroup.fgroup.FactorGroup)s.\n",
        "\n",
        "~~~python\n",
        "from pgmax import factor\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add unary factors\n",
        "for ii in range(bh.shape[0]):\n",
        "  unary_factor = factor.EnumFactor(\n",
        "      variables=[hidden_variables[ii]],\n",
        "      factor_configs=np.arange(2)[:, None],\n",
        "      log_potentials=np.array([0, bh[ii]]),\n",
        "  )\n",
        "  fg.add_factors(unary_factor)\n",
        "\n",
        "for jj in range(bv.shape[0]):\n",
        "  unary_factor = factor.EnumFactor(\n",
        "      variables=[visible_variables[jj]],\n",
        "      factor_configs=np.arange(2)[:, None],\n",
        "      log_potentials=np.array([0, bv[jj]]),\n",
        "  )\n",
        "  fg.add_factors(unary_factor)\n",
        "\n",
        "# Add pairwise factors\n",
        "factor_configs = np.array(list(itertools.product(np.arange(2), repeat=2)))\n",
        "for ii in tqdm(range(bh.shape[0])):\n",
        "  for jj in range(bv.shape[0]):\n",
        "    pairwise_factor = factor.EnumFactor(\n",
        "        variables=[hidden_variables[ii], visible_variables[jj]],\n",
        "        factor_configs=factor_configs,\n",
        "        log_potentials=np.array([0, 0, 0, W[ii, jj]]),\n",
        "    )\n",
        "    fg.add_factors(pairwise_factor)\n",
        "~~~\n",
        "\n",
        "Once we have added the factors, we can run max-product LBP and get the MAP decoding by\n",
        "~~~python\n",
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")\n",
        "bp_arrays = bp.run(bp.init(), num_iters=100, damping=0.5, temperature=0.0)\n",
        "beliefs = bp.get_beliefs(bp_arrays)\n",
        "map_states = infer.decode_map_states(beliefs)\n",
        "~~~\n",
        "or run sum-product LBP and get the estimated marginals by\n",
        "~~~python\n",
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")\n",
        "bp_arrays = bp.run(bp.init(), num_iters=100, damping=0.5, temperature=1.0)\n",
        "beliefs = bp.get_beliefs(bp_arrays)\n",
        "marginals = infer.get_marginals(beliefs)\n",
        "~~~\n",
        "More generally, PGMax implements LBP for any temperature, with `temperature=0.0` and `temperature=1.0` corresponding to the commonly used max/sum-product LBP respectively.\n",
        "\n",
        "Now we are ready to demonstrate PMP sampling from RBM. PMP perturbs the model with [Gumbel](https://numpy.org/doc/stable/reference/random/generated/numpy.random.gumbel.html) unary potentials, and draws a sample from the RBM as the MAP decoding from running max-product LBP on the perturbed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlpJouBKvxb_"
      },
      "outputs": [],
      "source": [
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pW25Wxp083z"
      },
      "outputs": [],
      "source": [
        "bp_arrays = bp.init(\n",
        "    evidence_updates={\n",
        "        hidden_variables: np.random.gumbel(size=(bh.shape[0], 2)),\n",
        "        visible_variables: np.random.gumbel(size=(bv.shape[0], 2)),\n",
        "    }\n",
        ")\n",
        "bp_arrays, msgs_deltas = bp.run_with_diffs(\n",
        "    bp_arrays, num_iters=500, damping=0.5, temperature=0.0\n",
        ")\n",
        "beliefs = bp.get_beliefs(bp_arrays)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3qQmcVc1AVF"
      },
      "source": [
        "Here we use the `evidence_updates` argument of `bp.init` to perturb the model with Gumbel unary potentials. In general, `evidence_updates` can be used to incorporate evidence in the form of externally applied unary potentials in PGM inference.\n",
        "\n",
        "Visualizing the MAP decoding, we see that we have sampled an MNIST digit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD6flvbP0-Gf"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "map_states = infer.decode_map_states(beliefs)\n",
        "ax.imshow(\n",
        "    map_states[visible_variables].copy().reshape((28, 28)),\n",
        "    cmap=\"gray\",\n",
        ")\n",
        "ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kMF8F6c7b3t"
      },
      "source": [
        "`bp.run_with_diffs` returns the maximum absolute difference of the messages between two consecutive iterations, which is used to monitor the (unguaranteed) convergence of BP\n",
        "\n",
        "If we do not want to access `msgs_deltas` we can also call \n",
        "\n",
        "`bp_arrays = bp.run(bp_arrays, num_iters=500, damping=0.5, temperature=0.0)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByZrOHuE7am-"
      },
      "outputs": [],
      "source": [
        "print(msgs_deltas[-10:])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(msgs_deltas)\n",
        "plt.title(\"Max-product convergence\", fontsize=18)\n",
        "plt.xlabel(\"BP iteration\", fontsize=16)\n",
        "plt.ylabel(\"Max abs msgs diff\", fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Godwj8XGrTG_"
      },
      "source": [
        "PGMax can compute the energy of a decoding, expressed by its MAP states.\n",
        "\n",
        "`debug_mode` gives access to the individual contribution of each variable and factor to the energy.\n",
        "\n",
        "~~~python\n",
        "decoding_energy, vars_energies, factors_energies = infer.compute_energy(\n",
        "    fg.bp_state, bp_arrays, map_states, debug_mode=True\n",
        ")\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMx3e4oQk70r"
      },
      "outputs": [],
      "source": [
        "decoding_energy = infer.compute_energy(fg.bp_state, bp_arrays, map_states)[0]\n",
        "print(\"The energy of the decoding is\", decoding_energy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOcR0GQr1Dz_"
      },
      "source": [
        "\n",
        "PGMax adopts a functional interface for implementing LBP: running LBP in PGMax starts with\n",
        "~~~python\n",
        "bp = infer.build_inferer(fg.bp_state, backend=\"bp\")\n",
        "~~~\n",
        "where the arguments of `bp` are several useful functions to run LBP. In particular, `bp.init`, `bp.run`, `bp.get_beliefs` are pure functions with no side-effects. This design choice means that we can easily apply JAX transformations like `jit`/`vmap`/`grad`, etc., to these functions, and additionally allows PGMax to seamlessly interact with other packages in the rapidly growing JAX ecosystem (see [here](https://deepmind.com/blog/article/using-jax-to-accelerate-our-research) and [here](https://github.com/n2cholas/awesome-jax)).\n",
        "\n",
        "As an example of applying `jax.vmap` to `bp.init`/`bp.run`/`bp.get_beliefs` to process a batch of samples/models in parallel, instead of drawing one sample at a time as above, we can draw a batch of samples in parallel as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY3blQHS1BzG"
      },
      "outputs": [],
      "source": [
        "n_samples = 10\n",
        "bp_arrays = jax.vmap(bp.init, in_axes=0, out_axes=0)(\n",
        "    evidence_updates={\n",
        "        hidden_variables: np.random.gumbel(size=(n_samples, bh.shape[0], 2)),\n",
        "        visible_variables: np.random.gumbel(size=(n_samples, bv.shape[0], 2)),\n",
        "    },\n",
        ")\n",
        "bp_arrays = jax.vmap(\n",
        "    functools.partial(bp.run, num_iters=100, damping=0.5, temperature=0.0),\n",
        "    in_axes=0,\n",
        "    out_axes=0,\n",
        ")(bp_arrays)\n",
        "\n",
        "beliefs = jax.vmap(bp.get_beliefs, in_axes=0, out_axes=0)(bp_arrays)\n",
        "map_states = infer.decode_map_states(beliefs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY8cS4CI1JiB"
      },
      "source": [
        "Visualizing the MAP decodings, we see that we have sampled 10 MNIST digits in parallel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA0FjuwZ1GDW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(20, 8))\n",
        "for ii in range(10):\n",
        "    ax[np.unravel_index(ii, (2, 5))].imshow(\n",
        "        map_states[visible_variables][ii].copy().reshape((28, 28)), cmap=\"gray\"\n",
        "    )\n",
        "    ax[np.unravel_index(ii, (2, 5))].axis(\"off\")\n",
        "\n",
        "fig.tight_layout()"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}
